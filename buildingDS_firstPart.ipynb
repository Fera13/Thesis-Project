{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from kneed import KneeLocator\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "dotenv_path = os.path.join(current_file_directory, '.env')\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Access the environment variable\n",
    "my_api_key = os.getenv(\"GOOGLE_KEY\")\n",
    "\n",
    "gmaps = googlemaps.Client(key=my_api_key)\n",
    "\n",
    "\n",
    "def load_distance_cache():\n",
    "    try:\n",
    "        with open(\"distance_cache.json\", \"r\") as file:\n",
    "            loaded_cache = json.load(file)\n",
    "            # Convert string keys back to tuples if your logic requires tuple keys\n",
    "            return {tuple(key.split(\",\")): value for key, value in loaded_cache.items()}\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return {}  # Return an empty dictionary if there is no file or decoding fails\n",
    "\n",
    "\n",
    "def save_distance_cache(cache):\n",
    "    with open(\"distance_cache.json\", \"w\") as file:\n",
    "        # Convert tuple keys to a string format\n",
    "        formatted_cache = {\",\".join(key): value for key, value in cache.items()}\n",
    "        json.dump(formatted_cache, file, indent=4)\n",
    "\n",
    "\n",
    "# Make sure to call this once at the start of your program or script\n",
    "distance_cache = load_distance_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cities_csv_to_dict(csv_file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file containing city information and returns a dictionary\n",
    "    with city names as keys and their coordinates (latitude and longitude) as values.\n",
    "\n",
    "    Args:\n",
    "    csv_file_path: The file path to the CSV file containing the city data.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary with city names as keys and (latitude, longitude) tuples as values.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path, delimiter=\",\")\n",
    "\n",
    "    duplicates = df[df.duplicated(keep=False)]\n",
    "    if duplicates.empty:\n",
    "        print(\"No duplicates found.\")\n",
    "    else:\n",
    "        print(\"Duplicates found:\", len(duplicates))\n",
    "        df.drop_duplicates(keep=\"first\", inplace=True)\n",
    "\n",
    "    # Create a dictionary from the DataFrame\n",
    "    city_coordinates = {\n",
    "        row[\"city\"]: (row[\"lat\"], row[\"lng\"])\n",
    "        for index, row in df.iterrows()\n",
    "        if row[\"country\"] == \"Sweden\"\n",
    "    }\n",
    "    print(city_coordinates)\n",
    "\n",
    "    return city_coordinates\n",
    "\n",
    "\n",
    "csv_file_path = \"Sweden_cities.csv\"\n",
    "cities_coordinates = read_cities_csv_to_dict(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code inspired from https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd\n",
    "# and https://kneed.readthedocs.io/en/stable/parameters.html\n",
    "\n",
    "def calculate_eps(coordinates, min_samples=2):\n",
    "    \"\"\"\n",
    "    Automatically calculates the eps parameter for DBSCAN based on the nearest neighbors.\n",
    "\n",
    "    Args:\n",
    "    coordinates: List of (latitude, longitude) tuples for the cities.\n",
    "    min_samples: The minimum samples in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "    Returns:\n",
    "    The calculated eps value.\n",
    "    \"\"\"\n",
    "    # Use NearestNeighbors to find the distance to the nearest min_samples points\n",
    "    nn = NearestNeighbors(n_neighbors=min_samples)\n",
    "    nn.fit(coordinates)\n",
    "    distances, indices = nn.kneighbors(coordinates)\n",
    "\n",
    "    # Take the distance to the farthest of the min_samples points\n",
    "    distances = np.sort(distances[:, min_samples - 1], axis=0)\n",
    "\n",
    "    # Find the \"knee\" in the distances graph which is a good estimate for eps\n",
    "    knee_locator = KneeLocator(\n",
    "        range(len(distances)), distances, curve=\"convex\", direction=\"increasing\"\n",
    "    )\n",
    "    eps = distances[knee_locator.knee] if knee_locator.knee else np.mean(distances)\n",
    "    return eps\n",
    "\n",
    "    # Uncomment and move inside the function to Plot the nearest distances with the knee point\n",
    "\"\"\"     plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(distances)), distances, marker=\"o\")\n",
    "    if knee_locator.knee is not None:\n",
    "        plt.vlines(knee_locator.knee, plt.ylim()[0], plt.ylim()[1], linestyles=\"dashed\")\n",
    "        plt.scatter(\n",
    "            knee_locator.knee,\n",
    "            distances[knee_locator.knee],\n",
    "            color=\"red\",\n",
    "            s=100,\n",
    "            zorder=5,\n",
    "            label=f\"Knee at index {knee_locator.knee}, eps={eps:.3f}\",\n",
    "        )\n",
    "    plt.title(\"Elbow Method for Determining Optimal eps\")\n",
    "    plt.xlabel(\"Points sorted by distance to min_samples-th nearest neighbor\")\n",
    "    plt.ylabel(\"Distance to min_samples-th nearest neighbor\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perimeter_and_area(points):\n",
    "    \"\"\"\n",
    "    Calculate the perimeter of the convex hull of a set of points.\n",
    "\n",
    "    Args:\n",
    "    points: An array of points in the format [(x1, y1), (x2, y2), ...]\n",
    "\n",
    "    Returns:\n",
    "    The perimeter of the convex hull of the given points.\n",
    "    \"\"\"\n",
    "    if len(points) < 3:\n",
    "        # Not enough points to form a convex hull; return 0\n",
    "        return np.nan, np.nan  # Use NaN to indicate the value is not available\n",
    "\n",
    "    # Ensure all points do not lie on a single line or are not identical\n",
    "    if np.std(points[:, 0]) == 0 or np.std(points[:, 1]) == 0:\n",
    "        return np.nan, np.nan  # Points are collinear or identical in one dimension\n",
    "\n",
    "    try:\n",
    "        hull = ConvexHull(points)\n",
    "        perimeter = hull.area\n",
    "        area = hull.volume\n",
    "        return perimeter, area\n",
    "    except Exception as e:\n",
    "        print(\"Failed to compute ConvexHull:\", e)\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\n",
    "\n",
    "def dbscan_and_metrics(coordinates):\n",
    "    \"\"\"\n",
    "    Performs DBSCAN clustering on the provided coordinates and calculates various metrics for each cluster.\n",
    "\n",
    "    This function automatically calculates the 'eps' parameter for DBSCAN using the nearest neighbors approach,\n",
    "    performs the clustering, and then calculates metrics such as silhouette score, cluster sizes, inter-cluster distances,\n",
    "    and various geometric properties of the clusters like perimeter and area.\n",
    "\n",
    "    Args:\n",
    "        coordinates (dict): A dictionary with city names as keys and (latitude, longitude) tuples as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'cluster_IDs': An array of cluster labels for each point.\n",
    "            - 'cluster_sizes': A list with the size (number of points) of each cluster.\n",
    "            - 'n_clusters': The number of clusters found, excluding noise.\n",
    "            - 'avg_inter_cluster_distance_km': The average distance between clusters in kilometers.\n",
    "            - 'min_inter_cluster_distance_km': The minimum distance between any two clusters in kilometers.\n",
    "            - 'max_inter_cluster_distance_km': The maximum distance between any two clusters in kilometers.\n",
    "            - 'average_silhouette': The average silhouette score across all clusters.\n",
    "            - 'n_noise_points': The number of points classified as noise.\n",
    "            - 'std_dev_cluster_sizes': The standard deviation of the sizes of the clusters.\n",
    "            - 'average_cluster_density': The average density of clusters, defined as size/area.\n",
    "            - 'average_cluster_perimeter': The average perimeter of the clusters.\n",
    "            - 'average_cluster_area': The average area of the clusters.\n",
    "            - 'average_cluster_complexity': An average measure of cluster complexity, defined as perimeter/sqrt(area).\n",
    "    \"\"\"\n",
    "    if not coordinates:\n",
    "        print(\"No coordinates provided for clustering.\")\n",
    "        return {}\n",
    "\n",
    "    # Convert city coordinates to a NumPy array for DBSCAN\n",
    "    X = np.array(list(coordinates.values()))\n",
    "    if X.size == 0:\n",
    "        print(\"Empty coordinate array.\")\n",
    "        return {}\n",
    "\n",
    "    min_sample = 4  # based on https://www.theaidream.com/post/dbscan-clustering-algorithm-in-machine-learning 2.dim\n",
    "    start_time_eps = time.time()\n",
    "    # Calculate eps automatically\n",
    "    eps = calculate_eps(X, min_samples=min_sample)\n",
    "    execution_time_eps = time.time() - start_time_eps\n",
    "\n",
    "    # Perform DBSCAN clustering\n",
    "    db = DBSCAN(eps=eps, min_samples=min_sample, metric=\"euclidean\").fit(X)\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters, excluding noise if present\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    if n_clusters_ == 0:\n",
    "        print(\"No clusters found.\")\n",
    "        return {\n",
    "            \"DBSCAN_min_sample\": min_sample,\n",
    "            \"DBSCAN_eps\": eps,\n",
    "            \"cluster_IDs\": [],\n",
    "            \"cluster_sizes\": [],\n",
    "            \"n_clusters\": 0,\n",
    "            \"n_noise_points\": n_noise_,\n",
    "            \"avg_inter_cluster_distance_km\": np.nan,\n",
    "            \"min_inter_cluster_distance_km\": np.nan,\n",
    "            \"max_inter_cluster_distance_km\": np.nan,\n",
    "            \"average_silhouette\": np.nan,\n",
    "            \"std_dev_cluster_sizes\": std_dev_cluster_sizes,\n",
    "            \"avg_cluster_density\": np.nan,\n",
    "            \"avg_cluster_perimeter\": np.nan,\n",
    "            \"avg_cluster_area\": np.nan,\n",
    "            \"avg_cluster_complexity\": np.nan,\n",
    "            \"eps_exec_time\": execution_time_eps,\n",
    "        }\n",
    "    elif n_clusters_ > 1:\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "    else:\n",
    "        silhouette_avg = np.nan  # silhouette score is not meaningful with 1 or 0 clusters\n",
    "\n",
    "    # Prepare cluster information\n",
    "    clusters = [X[labels == i] for i in range(n_clusters_)]\n",
    "    cluster_sizes = [len(cluster) for cluster in clusters]\n",
    "    std_dev_cluster_sizes = np.std(cluster_sizes) if cluster_sizes else 0\n",
    "\n",
    "    # Calculate inter-cluster distances\n",
    "    cluster_centers = [np.mean(cluster, axis=0) for cluster in clusters]\n",
    "    if len(cluster_centers) > 1:\n",
    "        inter_cluster_distances = pdist(cluster_centers, 'euclidean') * 111  # Approx. conversion from degrees to km\n",
    "        avg_inter_cluster_distance = np.mean(inter_cluster_distances)\n",
    "        min_inter_cluster_distance = np.min(inter_cluster_distances)\n",
    "        max_inter_cluster_distance = np.max(inter_cluster_distances)\n",
    "    else:\n",
    "        avg_inter_cluster_distance = min_inter_cluster_distance = max_inter_cluster_distance = np.nan\n",
    "\n",
    "    cluster_perimeters, cluster_areas = zip(\n",
    "        *[calculate_perimeter_and_area(cluster) for cluster in clusters]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"DBSCAN_min_sample\": min_sample,\n",
    "        \"DBSCAN_eps\": eps,\n",
    "        \"cluster_IDs\": labels,\n",
    "        \"cluster_sizes\": cluster_sizes,\n",
    "        \"n_clusters\": n_clusters_,\n",
    "        \"n_noise_points\": n_noise_,\n",
    "        \"avg_inter_cluster_distance_km\": avg_inter_cluster_distance,\n",
    "        \"min_inter_cluster_distance_km\": min_inter_cluster_distance,\n",
    "        \"max_inter_cluster_distance_km\": max_inter_cluster_distance,\n",
    "        \"avg_silhouette\": silhouette_avg,\n",
    "        \"std_dev_cluster_sizes\": std_dev_cluster_sizes,\n",
    "        \"avg_cluster_density\": np.mean(\n",
    "            [\n",
    "                size / area if area else 0\n",
    "                for size, area in zip(cluster_sizes, cluster_areas)\n",
    "            ]\n",
    "        ),\n",
    "        \"avg_cluster_perimeter\": (\n",
    "            np.mean(cluster_perimeters) if cluster_perimeters else np.nan\n",
    "        ),\n",
    "        \"avg_cluster_area\": np.mean(cluster_areas) if cluster_areas else np.nan,\n",
    "        \"avg_cluster_complexity\": np.mean(\n",
    "            [\n",
    "                perimeter / np.sqrt(area) if area else 0\n",
    "                for perimeter, area in zip(cluster_perimeters, cluster_areas)\n",
    "            ]\n",
    "        ),\n",
    "        \"eps_exec_time\": execution_time_eps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call code inspired from https://medium.com/how-to-use-google-distance-matrix-api-in-python/how-to-use-google-distance-matrix-api-in-python-ef9cd895303c\n",
    "# and https://www.linkedin.com/pulse/calculating-distances-using-python-google-maps-r%C3%A9gis-nisengwe/\n",
    "\n",
    "\n",
    "def fetch_distances(selected_cities):\n",
    "    \"\"\"\n",
    "    Fetches the driving distances between each pair of selected cities using Google Maps API.\n",
    "    Updates and uses a local cache to minimize API calls.\n",
    "    \"\"\"\n",
    "    print(\"started fetch_distances\")\n",
    "    distances = {}\n",
    "    cache_updated = False\n",
    "    for origin in selected_cities:\n",
    "        distances[origin] = {}\n",
    "        for destination in selected_cities:\n",
    "            if origin == destination:\n",
    "                distances[origin][destination] = 0\n",
    "            else:\n",
    "                # Sort and convert to string to use as a JSON-compatible key\n",
    "                cache_key = tuple(sorted([origin, destination]))\n",
    "                str_cache_key = \",\".join(cache_key)\n",
    "                if cache_key in distance_cache:\n",
    "                    distances[origin][destination] = distance_cache[cache_key]\n",
    "                    print(\"Found cache\")\n",
    "                else:\n",
    "                    try:\n",
    "                        print(\"started API call\")\n",
    "                        result = gmaps.distance_matrix(\n",
    "                            origins=[cities_coordinates[origin]],\n",
    "                            destinations=[cities_coordinates[destination]],\n",
    "                            mode=\"driving\",\n",
    "                        )\n",
    "                        distance = result[\"rows\"][0][\"elements\"][0][\"distance\"][\"value\"]\n",
    "                        distances[origin][destination] = distance\n",
    "                        distance_cache[cache_key] = distance  # Store using tuple\n",
    "                        cache_updated = True\n",
    "                        print(\"finished API call\")\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"Error fetching distance between {origin} and {destination}: {e}\"\n",
    "                        )\n",
    "                        distances[origin][destination] = float(\n",
    "                            \"inf\"\n",
    "                        )  # Assign a high cost in case of error\n",
    "    if cache_updated:\n",
    "        save_distance_cache(distance_cache)\n",
    "    print(\"finished fetch_distances\")\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(solution, distances):\n",
    "    \"\"\"\n",
    "    Calculates the total travel cost of a given solution based on the distances between cities.\n",
    "\n",
    "    Args:\n",
    "        solution (list): An ordered list of city names representing the tour.\n",
    "        distances (dict): A nested dictionary of distances between each pair of cities.\n",
    "\n",
    "    Returns:\n",
    "        float: The total cost of the solution in kilometers.\n",
    "    \"\"\"\n",
    "    # Calculate the total distance in meters first\n",
    "    total_distance_meters = (\n",
    "        sum(distances[solution[i]][solution[i + 1]] for i in range(len(solution) - 1))\n",
    "        + distances[solution[-1]][solution[0]]\n",
    "    )\n",
    "    # Convert the total distance to kilometers\n",
    "    total_distance_km = total_distance_meters / 1000.0\n",
    "    return total_distance_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://jupyter.brynmawr.edu/services/public/dblank/jupyter.cs/FLAIRS-2015/TSPv3.ipynb\n",
    "\n",
    "def nearest_neighbor(cities, distances):\n",
    "    \"\"\"\n",
    "    Finds a tour by repeatedly visiting the nearest unvisited city.\n",
    "\n",
    "    Args:\n",
    "        cities (list): A list of city names to visit.\n",
    "        distances (dict): A nested dictionary of distances between each pair of cities.\n",
    "\n",
    "    Returns:\n",
    "        list: An ordered list representing the tour of cities.\n",
    "    \"\"\"\n",
    "    print(\"started nearest_neighbor\")\n",
    "    start_city = random.choice(cities)\n",
    "    unvisited = set(cities)\n",
    "    unvisited.remove(start_city)\n",
    "    tour = [start_city]\n",
    "    while unvisited:\n",
    "        last_city = tour[-1]\n",
    "        next_city = min(unvisited, key=lambda city: distances[last_city][city])\n",
    "        unvisited.remove(next_city)\n",
    "        tour.append(next_city)\n",
    "    print(\"finished nearest_neighbor\")\n",
    "    return tour"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
